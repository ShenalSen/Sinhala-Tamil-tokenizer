{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer**\n",
    "-------------\n",
    "\n",
    "**What is a Tokenizer?**\n",
    "\n",
    "A tokenizer is like a tool that breaks down text into smaller, manageable pieces called tokens. Tokens can be words, subwords, or even characters. For example, if you have the sentence \"I love programming,\" a tokenizer might split it into tokens like:\n",
    "\n",
    "    Word tokens: [\"I\", \"love\", \"programming\"]\n",
    "    Char tokens: [\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"p\", \"r\", \"o\", \"g\", \"r\", \"a\", \"m\", \"m\", \"i\", \"n\", \"g\"]\n",
    "\n",
    "These tokens are the basic units of text that a computer can work with when performing tasks like language processing or model training.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the Role of a Tokenizer in Building a Large Language Model (LLM)?**\n",
    "\n",
    "When you’re creating a Large Language Model (LLM), such as the ones used in chatbots, translations, or text generation, the model doesn't directly understand full sentences or words. It processes tokens instead. Here’s why the tokenizer is important:\n",
    "\n",
    "    1. Breaking Down Text: Text comes in as sentences or paragraphs, but an LLM needs to process it in smaller chunks. The tokenizer breaks the text into tokens, which the model can then learn from.\n",
    "\n",
    "    2. Mapping to Numbers: Computers understand numbers, not words. The tokenizer assigns each token a unique number (an ID), creating a mapping between the words and numbers. For example, the word \"I\" might be mapped to the number 5, \"love\" to 12, and \"programming\" to 53.\n",
    "\n",
    "    3. Handling Out-of-Vocabulary Words: In languages, we often have new or unfamiliar words. A good tokenizer breaks down unknown words into smaller parts (like subwords or characters) so the model can still understand them.\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Why Do We Need to Create a Sinhala Tokenizer for This Task?**\n",
    "\n",
    "Since you’re working on building an LLM for Sinhala/Tamil, here’s why a tokenizer specific to Sinhala is important:\n",
    "\n",
    "    1. Language-Specific Rules: Sinhala has its own set of characters, rules for how words are formed, and punctuation. A generic tokenizer designed for English won’t handle Sinhala properly. For example, it might not recognize Sinhala letters or might split words incorrectly. A Sinhala-specific tokenizer is designed to understand these special rules.\n",
    "\n",
    "    2. Improving Model Performance: When you train an LLM on Sinhala text, you want the model to learn meaningful patterns from the language. A properly trained tokenizer ensures that the text is broken down into tokens that make sense for Sinhala, improving the model's ability to generate, understand, and work with Sinhala language.\n",
    "\n",
    "    3. Handling Complex Words: Sinhala has compound words and affixes that modify words. A good tokenizer will break down these words into meaningful subwords or units, allowing the model to better capture the richness of the language.\n",
    "\n",
    "    4. Efficient Learning: The tokenizer helps the model focus on learning the structure of the language rather than getting confused by large chunks of unprocessed text. For example, the word \"අනේ\" might be treated as a single token, and the model will know exactly what it means without breaking it into random parts.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Now Let's create a simple Tokenizer*\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to Create and Train a Sinhala Tokenizer**\n",
    "\n",
    "*Step 1: Install Required Libraries*\n",
    "    \n",
    "To get started, you’ll need to install ```sentencepiece```, which is a popular library for creating tokenizers, especially for tasks like training large language models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: Get a Large Sinhala Text Corpus*\n",
    "\n",
    "To train a tokenizer, you need a lot of text in Sinhala. This text will help the tokenizer learn how to split sentences into meaningful tokens. You can gather text from:\n",
    "\n",
    "    Sinhala Wikipedia\n",
    "    Sinhala news websites\n",
    "    Sinhala books (if you can find any in digital format)\n",
    "    Save this large collection of text as a .txt file, say sinhala_corpus.txt.\n",
    "\n",
    "*Step 3: Train the Tokenizer Using SentencePiece*\n",
    "\n",
    "Now, use SentencePiece to train your tokenizer. The idea is to build a model that knows how to break Sinhala text into tokens like words, subwords, or characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train the SentencePiece model with a smaller vocabulary size\n",
    "spm.SentencePieceTrainer.train(\n",
    "    '--input=sinhala_corpus.txt --model_prefix=sinhala_tokenizer --vocab_size=4195 --model_type=bpe'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "    --input=sinhala_corpus.txt:\n",
    "This is the path to your Sinhala text file.\n",
    "\n",
    "    --model_prefix=sinhala_tokenizer:\n",
    "This specifies the prefix for the output files (the trained model and vocabulary). You will get two files: sinhala_tokenizer.model and sinhala_tokenizer.vocab.\n",
    "\n",
    "    --vocab_size=32000:\n",
    "This sets the size of your vocabulary (the number of unique tokens). You can adjust this depending on your needs.\n",
    "\n",
    "    --model_type=bpe:\n",
    "This specifies the type of tokenizer you want to build. BPE (Byte-Pair Encoding) is a common choice for subword tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "***Use the Trained Tokenizer***\n",
    "\n",
    "Once the tokenizer is trained, you can use it to tokenize new Sinhala text. Here’s how you can do that in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the trained tokenizer model\n",
    "sp = spm.SentencePieceProcessor(model_file='sinhala_tokenizer.model')\n",
    "\n",
    "# Sample Sinhala sentence\n",
    "sinhala_sentence = \"සංජුල ඔබට කෙසේද? මම නම් කාර්යබහුලයි.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = sp.encode(sinhala_sentence, out_type=str)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Use the Tokenizer to Train an LLM**\n",
    "\n",
    "    After creating your tokenizer, you can use it to preprocess text data for training a Large Language Model (LLM). The tokens generated by the tokenizer will be fed into the LLM, helping it learn the structure and patterns of Sinhala.\n",
    "\n",
    "**Step 5: Fine-Tune or Train Models**\n",
    "\n",
    "    Once your tokenizer is ready and you have tokenized your text, you can use popular deep learning libraries like TensorFlow or PyTorch to train your LLM using architectures like BERT, GPT, or Transformers.\n",
    "\n",
    "**Here’s a basic overview of how you might proceed:**\n",
    "\n",
    "    1. Tokenize Text: Use the trained tokenizer to convert Sinhala sentences into tokens.\n",
    "    2. Feed Tokens to the LLM: Train your model on these tokens.\n",
    "    3. Evaluate: Test the model on unseen Sinhala text to see how well it understands and generates Sinhala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "\n",
    "***Example of Connecting the Tokenizer to an LLM in Python***\n",
    "\n",
    "-------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:\\F-DRIVE\\GIT\\UCSC LLM\\Train Tokenizer\\sinhala_tokenizer.model\") #path/to/sinhala_tokenizer.model\n",
    "\n",
    "# Tokenize input text\n",
    "sinhala_sentence = \"සංජුල ඔබට කෙසේද? මම නම් කාර්යබහුලයි.\"\n",
    "tokens = tokenizer(sinhala_sentence)\n",
    "print(\"Token IDs:\", tokens['input_ids'])\n",
    "\n",
    "# Load a pretrained LLM (for example, a BERT model)\n",
    "model = AutoModel.from_pretrained(\"C:/F-DRIVE/GIT/UCSC LLM/Pretrained Models/sinhala_bert\") #path/to/pretrained_sinhala_model\n",
    "\n",
    "# Pass the token IDs into the LLM\n",
    "outputs = model(**tokens)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "***Summary of Files:*** \n",
    "\n",
    "```.model file:```  The core file that connects your tokenizer to the LLM. This is where the tokenization logic is stored.\n",
    "\n",
    "```.vocab file:``` The vocabulary file is optional in some frameworks, as the .model file often handles both tokenization and vocabulary mapping. However, it can be referenced when needed to map tokens to IDs.\n",
    "\n",
    "***Recap of Workflow:***\n",
    "\n",
    "1. Tokenizer's .model file: Used to preprocess the text (split into tokens and convert to numerical IDs).\n",
    "2. LLM: Processes the numerical IDs, learns patterns, and generates responses.\n",
    "Tokenizer: Converts the output back to readable text.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
