{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 1:Install Necessary Libraries\n",
    "\n",
    "You might want to use libraries like nltk (Natural Language Toolkit) for handling tokenization. You can install it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2: Write a Basic Tokenizer for Sinhala/Tamil\n",
    "A simple tokenizer splits text into words based on spaces or punctuation. Since Sinhala and Tamil have specific punctuation rules, this basic version will handle standard punctuation and spaces.\n",
    "\n",
    "Here’s a basic Python tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinhala Tokens: ['ස', 'ජ', 'ල', 'ඔබට', 'ක', 'ස', 'ද', 'මම', 'නම', 'ක', 'ර', 'යබහ', 'ලය']\n",
      "Tamil Tokens: ['ந', 'ங', 'கள', 'எப', 'பட', 'இர', 'க', 'க', 'ற', 'ர', 'கள', 'ந', 'ன', 'ப', 'ஸ', 'ய', 'க', 'இர', 'க', 'க', 'ற', 'ன']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A sample text in Sinhala\n",
    "sinhala_text = \"සංජුල ඔබට කෙසේද? මම නම් කාර්යබහුලයි.\"\n",
    "\n",
    "# A sample text in Tamil\n",
    "tamil_text = \"நீங்கள் எப்படி இருக்கிறீர்கள்? நான் பிஸியாக இருக்கிறேன்.\"\n",
    "\n",
    "# Basic word-based tokenizer function\n",
    "def basic_tokenizer(text):\n",
    "    # Split words based on spaces and remove punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the Sinhala text\n",
    "sinhala_tokens = basic_tokenizer(sinhala_text)\n",
    "print(\"Sinhala Tokens:\", sinhala_tokens)\n",
    "\n",
    "# Tokenize the Tamil text\n",
    "tamil_tokens = basic_tokenizer(tamil_text)\n",
    "print(\"Tamil Tokens:\", tamil_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 3: Explanation of Tokenization Process\n",
    "\n",
    "    Regex: The regular expression \\b\\w+\\b is used to match word boundaries, ensuring that the text is split into words and symbols are removed.\n",
    "Output: You’ll see the input sentence broken down into individual tokens.\n",
    "\n",
    "\n",
    "<--- The issue arises because your regular expression \\b\\w+\\b is tailored for Latin-based languages and doesn't handle complex scripts like Sinhala or Tamil correctly. In Sinhala and Tamil, characters often combine into ligatures or syllables, and those combined forms may not be recognized as single words by the regular expression.--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinhala Tokens: ['සංජුල', 'ඔබට', 'කෙසේද', 'මම', 'නම්', 'කාර්යබහුලයි']\n",
      "Tamil Tokens: ['நீங்கள்', 'எப்படி', 'இருக்கிறீர்கள்', 'நான்', 'பிஸியாக', 'இருக்கிறேன்']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A sample text in Sinhala\n",
    "sinhala_text = \"සංජුල ඔබට කෙසේද? මම නම් කාර්යබහුලයි.\"\n",
    "\n",
    "# A sample text in Tamil\n",
    "tamil_text = \"நீங்கள் எப்படி இருக்கிறீர்கள்? நான் பிஸியாக இருக்கிறேன்.\"\n",
    "\n",
    "# Basic word-based tokenizer function for Sinhala and Tamil\n",
    "def basic_tokenizer(text):\n",
    "    # Split words based on Unicode ranges for Sinhala and Tamil scripts\n",
    "    tokens = re.findall(r'[\\u0D80-\\u0DFF]+|[\\u0B80-\\u0BFF]+|\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the Sinhala text\n",
    "sinhala_tokens = basic_tokenizer(sinhala_text)\n",
    "print(\"Sinhala Tokens:\", sinhala_tokens)\n",
    "\n",
    "# Tokenize the Tamil text\n",
    "tamil_tokens = basic_tokenizer(tamil_text)\n",
    "print(\"Tamil Tokens:\", tamil_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 4: Convert Tokens to Numerical Representation\n",
    "\n",
    "    You can assign each token a unique index number. This is the simplest numerical representation, where each token corresponds to a unique integer.\n",
    "\n",
    "Here’s how you can implement this in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to Numerical Representation: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Token list from the previous step\n",
    "tokens = [\"සංජුල\",\"ඔබට\", \"කෙසේද\", \"මම\", \"නම්\", \"කාර්යබහුලයි\"]\n",
    "\n",
    "# Create a dictionary to assign an index to each token\n",
    "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
    "\n",
    "# Convert tokens to their numerical representation\n",
    "numerical_representation = [token_to_id[token] for token in tokens]\n",
    "\n",
    "print(\"Token to Numerical Representation:\", numerical_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#Step 2: Binary Representation of Tokens\n",
    "    \n",
    "    To convert these numerical values into binary (or a form that the computer can understand), we can use Python's built-in bin() function to get binary representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to Binary Representation: ['0b0', '0b1', '0b10', '0b11', '0b100', '0b101']\n"
     ]
    }
   ],
   "source": [
    "# Convert numerical representation to binary\n",
    "binary_representation = [bin(token_id) for token_id in numerical_representation]\n",
    "\n",
    "print(\"Token to Binary Representation:\", binary_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Word Embedding Model (Word2Vec)\n",
    "\n",
    "    For a more advanced approach, you can use Word2Vec, which converts each word (token) into a vector of real numbers (embedding). This captures the semantic meaning of words. To implement Word2Vec, you can use the gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample Sinhala/Tamil sentences tokenized\n",
    "sentences = [[\"සංජුල\",\"ඔබට\", \"කෙසේද\"], [\"මම\", \"නම්\", \"කාර්යබහුලයි\"]]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get vector representation of every token\n",
    "\n",
    "word_vector = model.wv[\"සංජුල\"]\n",
    "print(\"Vector representation for 'සංජුල':\", word_vector, \"\\n\")\n",
    "\n",
    "word_vector = model.wv[\"ඔබට\"]\n",
    "print(\"Vector representation for 'ඔබට':\", word_vector, \"\\n\")\n",
    "\n",
    "word_vector = model.wv[\"කෙසේද\"]\n",
    "print(\"Vector representation for 'කෙසේද':\", word_vector, \"\\n\")\n",
    "\n",
    "word_vector = model.wv[\"මම\"]\n",
    "print(\"Vector representation for 'මම':\", word_vector, \"\\n\")\n",
    "\n",
    "word_vector = model.wv[\"නම්\"]\n",
    "print(\"Vector representation for 'නම්':\", word_vector, \"\\n\")\n",
    "\n",
    "word_vector = model.wv[\"කාර්යබහුලයි\"]\n",
    "print(\"Vector representation for 'කාර්යබහුලයි':\", word_vector, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
